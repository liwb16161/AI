机器学习
一、概述
1.什么是机器学习
机器学习是一门能够让编程计算机从数据中学习的计算机科学。
一个计算机程序在完成任务T之后，获得经验E，其表现效果为P，如果随着任务的增加，其表征经验的效果也能随之增加，即P与T成正增长关系，这样的系统就是一个机器学习系统。
2.为什么需要机器学习
1)有助于提高系统的可维护性和可扩展性
2)用于解决算法非常复杂或没有算法的问题
经验主义
3)规则发现，算法自动生成，获得对业务的洞见
3.机器学习的类型
按照学习方式划分：有监督学习、无监督学习、半监督学习、强化学习
按照学习过程划分：批量学习、增量学习
按照学习策略划分：规则学习、实例学习
4.机器学习的基本问题
回归问题：根据输入预测分布于连续域中的输出。
1    2
2    4
3    6
3.5 7      y = 2 x
4.5 ? -> 9
分类问题：根据输入预测分布于离散域中的输出。
1    0
2    1
3    0      奇数: 0
4    1      偶数: 1
5    ? -> 0
聚类问题：根据样本特征的相似性将其划分为不同的族群
降维问题：在不影响学习效果的前提下减少样本的特征数
5.机器学习的基本流程
数据收集
数据清洗
数据预处理
选择模型
训练模型
验证模型
使用模型
维护模型
二、数据预处理
sci-kit learn(sklearn)经典机器学习框架
样本矩阵：一行一样本，一列一特征
输出向量：其中的每个元素与一个样本相对应
        输入特征              预测输出
年龄 工作时间 学历 ->    薪资
30          5      本科         8000
25          3      专科         6000
...
1.均值移除
将样本矩阵中的各个列(特征)的平均值调整为0，标准差调整为1，以此均化不同的特征，使它们对模型预测结果的贡献度趋于近似一致。
... 特征i ...
       a
       b
       c
m=(a+b+c)/3
a'=a-m
b'=b-m
c'=c-m
m'=(a'+b'+c')/3=(a-m+b-m+c-m)/3=(a+b+c)/3-m
=0
s'=sqrt((a'^2+b'^2+c'^2)/3)
a"=a'/s'
b"=b'/s'
c"=c'/s'
s"=sqrt((a"^2+b"^2+c"^2)/3)
=sqrt((a'^2+b'^2+c'^2)/(s'^2x3))
=sqrt((s'^2x3)/(s'^2x3))=1
import sklearn.preprocessing as sp
sp.scale(原始样本矩阵)->均值移除后的样本矩阵
代码：std.py
2.范围缩放
通过线性变换(y=kx+b)，使样本矩阵中每一列的最大值和最小值都相同，统一各个特征的分布范围。
... 特征i ...
       a -> min -> min'
       b
       c -> max -> max'
k min + b = min'
k max + b = max'
/ min 1 \ x / k \ = / min' \
\ max 1/    \ b /     \ max' /
----------    -----    ---------
     a               x             b
                      = np.linalg.solve(a, b)
范围缩放器=sp.MinMaxScaler(
    feature_range=(min', max'))
范围缩放器.fit_transform(原始样本矩阵)
    ->范围缩放后的样本矩阵
代码：mms.py
3.归一化
用每个样本的各个特征值占该样本所有特征值的绝对值的总和的比例来代替该特征值本身。
          Python Java C++ PHP
2017     10       20      5      2
2018     20       10     10     1
2019      5         2        1      0
           10/37
           20/41
             5/8
sp.normalize(原始样本矩阵, norm='l1')
    ->归一化后的样本矩阵
l1: l1范数，即矢量中个元素绝对值之和
l2: l2范数，即矢量中个元素平方之和
代码：nor.py
4.二值化
根据给定的阈值，将样本矩阵中的每一个特征值与该阈值进行比较，凡是大于该阈值者为1，否则为0，将样本矩阵简化为只由0和1组成的矩阵，以此来简化学习模型。
二值化器=sp.Binarizer(threshold=阈值)
二值化器.transform(原始样本矩阵)
    ->二值化后的样本矩阵
代码：bin.py
5.独热编码
1        3        2
7        5        4
1        8        6
7        3        9
1:10  3:100 2:1000
7:01  5:010 4:0100
          8:001 6:0010
                     9:0001
101001000
010100100
100010010
011000001
独热编码器=sp.OneHotEncoder(
    sparse=是否紧凑, dtype=元素类型)
独热编码器.fit_transform(原始样本矩阵)
    ->独热编码后的样本矩阵
代码：ohe.py
6.标签编码
将样本矩阵中的非数值列，按照字典序中的序号进行替换，以此把非数值形式的特征值数值化，便于在学习模型中参与运算。
标签编码器=sp.LabelEncoder()
标签编码器.fit_transform(原始非数值列)
    ->标签编码后的数值列
标签编码器.inverse_tansform(标签编码后的数值列)
    ->原始非数值列
代码：lab.py
三、梯度下降算法
输入 -> 学习模型 -> 预测输出  \ 尽可能
        \_  实际输出 _____________/ 的接近
0.5            5.0
0.6            5.5
0.8            6.0
1.1            6.8
1.4            7.0          预测函数 -> 寻找w0和w1 -> 模型参数
x                y             y'=w0+w1x
单样本误差：y-y'
总样本误差：1/2SIGMA[(y-y')^2]
损失函数：反映总样本误差因模型参数的变化而变化的规律loss=1/2SIGMA[(y-(w0+w1x))^2]
w0,w1 ? -> loss min
w0=w0+dw0=w0-nDloss/Dw0
w1=w1+dw1=w1-nDloss/Dw1
Dloss/Dw0=-SIGMA[y-(w0+w1x)]
Dloss/Dw1=-SIGMA[(y-(w0+w1x))x]
代码：gd.py
四、线性回归
import sklearn.linear_model as lm
创建学习模型对象：model=lm.LinearRegression()
训练学习模型对象：model.fit(x, y) # [x, y]-BGD->[w0, w1]
预测给定输入的输出：pred_y = model.predict(pred_x)
代码：line.py、load.py
五、岭回归
普通的线性回归，在计算总样本误差即损失值时，对所有训练样本一视同仁，因此极少数"坏"样本会使得预测模型偏离于大多数好样本所遵循的规则，影响模型的预测精度。岭回归就是在线性回归的基础之上，为每个训练样本分配不同的权重，越是能够反应一般规律的大多数好样本所得到的权重越大，而极少数偏离于一般规律的坏样本则只能获得较低的权重，从而使得最终的预测模型尽可能偏向于多数好样本，而弱化少数坏样本对模型的影响。
                                                 ____超参数，人为给定
                                               /
model = lm.Ridge(正则强度/惩罚力度)
正则强度/惩罚力度：[0, oo)
正则强度越小，权重差异就越小，0表示无差异，等同线性回归
代码：rdg.py
六、多项式回归
y = w0+w1x+w2x^2+w3x^3+...+wnx^n
loss = Loss(w0,w1,...,wn)
y = w0+w1x1+w2x2+w3x3+...+wnxn
x1 -> x1,x2,x3,...,xn
                \_________/
                        |
   x1->多项式特征扩展-x1,x2,x3,...,xn->线性回归->w0~wn
            \________________________________________/
                                            |
                                         管线
import sklearn.pipeline as pl
import sklearn.preprocessing as sp
多项式特征扩展器=sp.PolynomialFeatures(n=最高次幂)
线性回归器=lm.LinearRegression()
管线模型=pl.make_pipeline(多项式特征扩展器,线性回归器)
管线模型.fit(x,y) # [x,y]-BGD->[w0,w1,w2,w3,...,wn]
管线模型.predict(x)->pred_y
欠拟合：过于简单的模型，或者训练集的规模过小，导致模型无法真实地反应输入和输出之间的规律，出现训练集和测试集的评估分值都比较低的现象。可以通过增加模型的复杂度，或者增加训练集的规模，提高模型的拟合度，优化其性能。
过拟合：过于复杂的模型，或者特征数过多，大致模型失去足够的一般性，即太过于倾向训练数据，反而对训练集以外的其它样本的预测性能大幅下降。可以减少特征数，或者降低模型的复杂度，在训练集和测试集的拟合程度上寻求一个折衷，提高模型的泛化能力。
代码：poly.py
七、决策树
既可用于解决回归问题，也可用于解决分类问题。
1.相似的输入必会产生相似的输出
年龄：0-青年，1-中年，2-老年
学历：0-大专，1-大本，2-硕士，3-博士
资历：0-小白，1-小牛，2-大牛，3-骨灰
性别：0-女性，1-男性
等级：0-低收入，1-中等收入，2-高收入
-------------------------------------
年龄  学历  资历  性别  月薪      等级
  0       1       0       1    6000       0
  0       0       1       1    7000       1
  1       2       2       1    10000     2
...
-----------------------------------------------------
  0      0       1        1    对输出取平均/对输出做投票
2.构建树状模型提高对相似输入的检索性能
依次选取总样本空间中的每一个特征作为划分子表的依据，将样本矩阵划分为若干层级的多个子矩阵，每一个层级对应一个特征，组成树状结构。预测时，根据待预测样本的每个特征值，找到与之对应的叶级子表，将该子表的输出按照平均或者投票的方式计算预测值。
3.优先选择对输出影响最大的部分特征划分子表
根据按照某个特征划分子表前后，其信息熵或基尼不纯度的减少量来判断该特征对输出的影响，信息熵或基尼不纯度减少量越大的特征，对输出的影响也越大，越应该优先作为子表划分的依据。
4.集成算法
1)自助聚合：每次从总样本空间中随机抽取一部分样本构建决策树，这样共构建B棵决策树
2)随机森林：每次从总样本空间中随机抽取一部分样本及特征构建决策树，这样共构建B棵决策树
3)正向激励：为样本空间中的每个样本分配初始权重，构建第一颗决策树，针对训练集中预测错误的样本，提升其权重，再构建第二棵决策树，以此类推，共构建B棵权重各不相同的决策树
代码：boston.py
5.特征重要性
决策树模型在确定子表划分依据的过程中，会计算按照每个特征划分子表所引起的信息熵或基尼不纯度减少量，从业务上看该指标即体现了，每个特征对输出的影响力度。
model = ...
model.fit(...)
model.feature_importances_
代码：fi.py
特征重要性与模型的算法有关，还与数据的粒度有关。
代码：bike.py
----------------
对于回归问题，模型关注的是回归曲线，该曲线反映了输入数据和输出数据之间的函数关系。
对于分类问题，模型关注的是分类边界，边界线反映了不同类别之间的划分依据。
----------------
八、简单分类
  x  -> y
3 1      0
2 5      1
1 8      1
6 4      0
5 2      0
3 5      1
4 7      1
4-1     0   x1>x2: y = 0 \  x1 = x2
---------   x1<x2: y =1  /
3  9     ? -> 1
代码：simple.py
九、逻辑回归分类器
y = w0+w1x
               1
z = ------------, x样本被归属为1类别的概率
       1 + e^-y
梯度下降
model=LogisticRegression(
    solver='liblinear', C=正则强度)
w0+w1x1+w2x2=0
代码：log.py
xxxx A 1    0     0
xxxx B 0    1     0
xxxx C 0    0     1
            A    B     C
........   0.2  0.6  0.4   B
........   0.6  0.1  0.3   A
........   0.4  0.2  0.8   C
代码：log2.py
十、朴素贝叶斯分类
1 2 3 -> 0
4 5 6 -> 1
7 8 9 -> 2
...
1 5 9 -> ? 0  0.7
                  1  0.5
                  2  0.8 *
P(x1,x2,x3,C)
=P(x1|x2,x3,C)P(x2,x3,C)
=P(x1|x2,x3,C)P(x2|x3,C)P(x3,C)
=P(x1|x2,x3,C)P(x2|x3,C)P(x3|C)P(C)
朴素：条件独立，特征值之间没有约束性。
=P(x1|C)P(x2|C)P(x3|C)P(C)
将特定类别出现的概率与该类别出现时每一个特征值出现的概率取乘积，以此表示该组特征值被归属为该类别的概率。以此计算该组特征值被归属为每一个类别的概率，择其最大的概率所对应的类别作为预测结果。
关于某个特征值在特定类别出现时的概率，可以通过事先已知的概率密度函数或概率质量函数计算得到。
体现历史数据所表现出的统计规律，同时不存在对分类边界的线性约束，但是对于统计规则不明且样本数量较少的场合不适用。
import sklearn.naive_bayes as nb
model = nb.GaussianNB()
基于高斯分布即正态分布的朴素贝叶斯分类器
代码：nb.py
1.划分训练集和测试集
import sklearn.model_selection as ms;
ms.train_test_split(
    输入集, 输出集, test_size=测试集比例,
    random_state=随机种子)->
    训练输入集，测试输入集，训练输出集，测试输出集
该函数对每一个类别单独洗牌，然后根据test_size参数提取相应比例的样本用于测试，其余样本用于训练，各个类别的分布比例在训练集和测试集中都是一致的。
代码：split.py
2.评价分类器的性能
查准率：对于某个类别，找对的/找出来的，正确性。
召回率：对于某个类别，找对的/该类别样本数，完整性。
F1得分：2x查准率x召回率/(查准率+召回率)
3.交叉验证
ms.cross_val_score(模型对象, 输入集, 输出集,
    cv=验证次数, scoring=评价指标)->
    每次验证测试集各个类别指标的平均值
代码：cv.py
4.混淆矩阵
import sklearn.metrics as sm
sm.confusion_matrix(实际输出, 预测输出)->混淆矩阵
每一行对应一个实际类别
每一列对应一个预测类别
对角线上元素表示各个类别分类正确的样本数，其它位置的元素表示分类误差。
对角线上元素/其所在列元素之和 = 该类别的查准率 \ F1得分
对角线上元素/其所在行元素之和 = 该类别的召回率 /
代码：cm.py
5.分类报告
针对每一个类别的评价指标及平均值。
sm.classification_report(实际输出, 预测输出)->分类报告
代码：cr.py
十一、决策树分类
投票
基于随机森林分类器的汽车品质评估
代码：car.py
1.验证曲线
模型的交叉验证得分=f(模型超参数)
ms.validation_curve(
    模型对象, 输入集, 输出集, 超参数名, 超参数表, cv=验证次数)
    ->训练集得分矩阵, 测试集得分矩阵
                     验证1 验证2 ...
超参数取值1                          -> 平均
超参数取值2
...
代码：vc.py
2.学习曲线
模型的交叉验证得分=f(训练集大小)
ms.learning_curve(
    模型对象, 输入集, 输出集, 训练集大小列表, cv=验证次数)
    ->训练集大小列表, 训练集得分矩阵, 测试集得分矩阵
代码：lc.py
十二、支持向量机
1.原理
寻求最优的分类边界，即被支持向量所夹持的分类带宽度达到最大值，取其中心线作为分类边界。
1)安全性：分类带最宽
2)公平性：分类带中心线
3)简单性：线性边界，分割超平面
4)对于在原始维度空间中无法线性分割的样本，通过特定的核函数增加特征，即升高维度，在高维度空间寻求分割超平面。
2.接口
import sklearn.svm as svm
分类器模型=svm.SVC(kernel=核函数类型, ...)
回归器模型=svm.SVR(kernel=核函数类型, ...)
3.线性核函数
代码：svm_line.py
4.多项式核函数
x1 x2
x1 x2 x1^2 x2^2 x1x2 x1^2x2 x1x2^2 x1^3 x2^3
代码：svm_poly.py
5.径向基核函数
代码：svm_rbf.py
6.样本均衡
svm.SVC(..., class_weight='balanced')
通过为不同类别的样本设置不同的权重，平衡比例相差较大的类别为分类器所带来的不同影响。
代码：svm_bal.py
7.置信概率
根据样本与分类边界的距离表示该样本被分类器归属每个类别的概率。
model = svm.SVC(..., probablity=True, ...)
model.predict_proba(输入集)->概率矩阵
概率矩阵中的一行对应输入集中的要给样本，一列表示一个的类别，而其中的值表示该样本被归属为特定类别的概率。
代码：svm_prob.py
8.网格搜索
超参数组合列表:
[{超参数名: [取值列表], 超参数名: [取值列表]}, {...}, ...]
model = ms.GridSearchCV(
    基本模型, 超参数组合列表, cv=交叉验证数)
model.fit(输入集, 输出集)
model -> 用最优超参数组合设置的模型
代码：svm_bhp.py
事件预测
代码：svm_evt.py
十三、聚类
在输出未知的前提下，仅根据已知的输入寻找样本之间的内在联系，据此将输入样本划分为不同的族群。
1.量化相似度
欧式距离
P(x1,y1)
Q(x2,y2)
|PQ|=sqrt((x1-x2)^2+(y1-y2)^2)
P(x1,y1,z1)
Q(x2,y2,z2)
|PQ|=sqrt((x1-x2)^2+(y1-y2)^2+(z1-z2)^2)
P(x1,y1,z1,...)
Q(x2,y2,z2,...)
张三(1.7,60)
李四(1.75,200)
王五(2.5,65)
赵六(1.72,61)
两个N维样本之间的欧氏距离越小，就越相似，反而反之。
2.K均值聚类
每个聚类都存在聚类中心，聚类中的样本围绕中心均匀分布。
每个聚类的几何中心（平均值）应该与该聚类的聚类中心重合。
首先聚类数K是已知的，然后随机选择K个样本作为初始聚类中心，计算每个样本与各个聚类中心的欧氏距离，选择距离最近的聚类中心作为样本的聚类归属，完成了一次划分。接着，计算每个聚类中所有样本的几何中心(各个特征的平均值)，如果所得到的几何中心与产生此聚类的聚类中心重合或足够接近，则完成聚类，否则以几何中心作为新的聚类中心，重复之前的聚类划分，再次计算几何中心，多次迭代以上过程，直到几何中心与聚类中心足够接近为止。
1)聚类数K必须事先知道。
2)因初始聚类中心选择不同，最后的聚类结果可能因之而不同。
代码：km.py
图像预处理之颜色量化
代码：quant.py
3.均值漂移聚类
将每个聚类中的样本看作是服从某种概率模型的随机分布，利用已知样本的统计直方图，拟合某个特定的概率模型，以概率密度的峰值点作为相应聚类的中心。然后，根据每个样本与聚类中心的距离，则其近者而从之，完成聚类划分。
1)无需事先给定聚类数
2)样本本身从业务上服从某种概率规律
代码：shift.py
4.凝聚层次聚类
首先假定每个样本都是一个独立的聚类，统计总聚类数，如果大于所要求的聚类数，就从每个样本出发，连接离它欧氏距离最近的样本，在扩大聚类的规模的同时减少聚类数，重复以上过程，直到总聚类数满足要求为止。
1)没有所谓聚类中心，适用于中心特性不明显的样本
2)无需事先给定聚类中心
3)在选择被凝聚样本的过程中，还可以分别按照距离优先和连续性优先两种方式选连接的样本。
代码：agglo.py、spiral.py
5.聚类的评价指标
内密外疏
对于每个样本计算内部距离a和外部距离b，得到该样本的轮廓系数s=(b-a)/max(a, b)，对所有样本的轮廓系数取平均值，即为整个样本空间的轮廓系数S=ave(s)。
内部距离a: 一个样本与同聚类其它样本的平均欧氏距离
外部距离b: 一个样本与离其聚类最近的另一个聚类中所有样本的平均欧氏距离。
代码：score.py
6.噪声密度聚类
代码：dbscan.py
十四、最近邻
代码：knnc.py、knnr.py
------------------------------------------------------------------
回归：线性、岭、多项式、决策树、SVM、KNN
R2得分
分类：逻辑、朴素贝叶斯、决策树、SVM、KNN
F1得分
聚类：K均值、均值漂移、凝聚层次、DBSCAN
轮廓系数
------------------------------------------------------------------
十五、推荐引擎
1.欧氏距离得分
X: [x1, x2, ..., xm]
Y: [y1, y2, ..., ym]
欧氏距离：sqrt((x1-y1)^2+(x2-y2)^2+...+(xm-ym)^2)
[0, oo)
欧氏距离得分 = 1/(1+欧氏距离)
(0,           1]
不相似  相似
             用户1    用户2    ...    用户7
用户1       1           0.5               0.3
用户2       ...            1
.
.
.
用户7
代码：es.py
2.皮氏距离得分
A 2-5
B 0-3
           电影1    电影2    电影3
用户1     5           3           4
用户2     3           0           1
[-1       0         1]
相反   无关   相同
代码：ps.py
根据相似程度排序
代码：sim.py
推荐清单
推荐度的计算
被推荐电影分值的加权平均
分值x相似度得分
代码：rcm.py
十六、自然语言
文本->语义
分类器
文本->特征值
今天 中午 我要 吃 饺子
NLTK - 自然语言工具包
1.分词
import nltk.tokenize as tk
tk.sent_tokenize(段落) -> 按句拆分
首字母大写、句尾标点(.!?...)
tk.word_tokenize(句子) -> 按单词拆分
n个连续<空格>、换行、标点
分词器 = tk.WordPunctTokenizer()
分词器.tokenize(句子)->按单词拆分
代码：tkn.py
2.词干提取
波特：偏宽松，保留更多的字母
兰卡斯特：偏严格，只保留较少的字母
思诺博：偏中庸，严格程度居于二者这间
词干!=词根!=原型
语义识别单位
代码：stm.py
3.词型还原
名词：变成单数
动词：动词原型
代码：lmm.py
4.词袋模型
词表：包含段落中不同单词的个数。
[1]The brown dog is running.
[2]The black dog is in the black room.
[3]Running in the room is forbidden.
the brown dog is running black in room forbidden
           black brown dog forbidden in is room running the
[1]         0         1         1           0         0  1    0           1         1
[2]         2         0         1           0         1  1    1           0         2
[3]         0         0         0           1         1  1    1           1         1 
代码：bow.py
词频
对词袋矩阵做归一化，用词表中的每个单词在每个样本中出现的频率，表示该单词对具体语句语义的价值。
代码：tf.py
                                    样本总数
逆文档频率 = -----------------------------
                       包含某个特定单词的样本数
词频逆文档频率：TF-IDF，自然语言的数学模型
代码：tfidf.py
5.文本分类
1 2 3 4 5 6
2 3 0 0 4 1
0 8 0 0 0 2
...
代码：doc.py
6.情感分析
一个样本一个tuple: (dict: {特征名: 特征值}, 输出)
整个样本集就是一个tuple的list
代码：sent.py
7.主题抽取
基于LDA，隐狄利克雷分布
代码：topic.py
十七、语音识别
声音(.wav)->文本(字符串)
1.声音的时域和频域表示
时域：位移=f(时间)
频域：(振幅, 相位)=f(频率)
代码：tf.py
2.梅尔频率倒谱系数(MFCC)矩阵
将一段音频样本划分成若干片段，其中每一个片段对应MFCC矩阵中的一行，构成一个子样本。将每个子样本做傅里叶变换得到频率谱线，从中选择与音频内容关系最为紧密的13个特征频率，形成一个特征向量。将多个子样本的特征向量组合成矩阵，即MFCC矩阵。
代码：mfcc.py
3.语音识别
代码：spch.py
十八、图像识别
业务数据--------->特征数据->学习模型
|              特征工程                ^
|   TFIDF、MFCC、SIFT      |
+----------------------------+
               深度学习
1.OpenCV基础
开源计算机视觉库
图像处理
提取图像特征
针对的图像的机器学习
代码：basic.py
2.边缘检测
代码：canny.py
3.亮度提升
直方图均衡化
代码：eq.py
4.角点检测
代码：corner.py
5.STAR特征检测
几何结构
代码：star.py
6.SIFT特征检测
突出亮度变化的方向
代码：sift.py
7.STAR-SIFT特征描述矩阵
通过对STAR特征点做进一步基于SIFT算法的筛选，以样本矩阵的形式表现的图像特征信息。
代码：desc.py
8.图像识别
类似的特征描述矩阵必然源自类似的图像
代码：obj.py
十九、人脸识别
1.视频捕捉
代码：vidcap.py
2.人脸定位
从整个画面中定位人脸的位置和范围































